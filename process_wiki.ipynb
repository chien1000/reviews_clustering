{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opencc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "from gensim.corpora import WikiCorpus\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/chien/Desktop/NTU/reviews_clustering/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.uce30ebc6fc60e24a5a955aa284681188.cache\n",
      "Loading model cost 1.276 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 設定繁體字典\n",
    "jieba.set_dictionary('dict.txt.big')\n",
    "with open('wikizh_terms_short.txt', 'r') as fin:\n",
    "    for line in fin.readlines():\n",
    "        line = line.strip()\n",
    "        jieba.add_word(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['這是', '一個', '簡單', '測試']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer_func(text: str, token_min_len: int, token_max_len: int, lower: bool) -> list of str`\n",
    "def jieba_token(text, token_min_len, token_max_len, lower):\n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "    text = opencc.convert(text, config='zhs2zht.ini')\n",
    "    tokens = jieba.cut(text)\n",
    "    tokens =[\n",
    "             gensim.utils.to_unicode(token) for token in tokens \n",
    "             if len(token) >= token_min_len and len(token) <= token_max_len and not token.startswith('_')\n",
    "            ]\n",
    "    return tokens\n",
    "jieba_token('這是一個简單的測試', 2,10,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_path = 'zhwiki-latest-pages-articles.xml.bz2'\n",
    "output_path= 'wiki.zh.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start!\n",
      "已處理 0 篇文章\n",
      "已處理 10000 篇文章\n",
      "已處理 20000 篇文章\n",
      "已處理 30000 篇文章\n",
      "已處理 40000 篇文章\n",
      "已處理 50000 篇文章\n",
      "已處理 60000 篇文章\n",
      "已處理 70000 篇文章\n",
      "已處理 80000 篇文章\n",
      "已處理 90000 篇文章\n",
      "已處理 100000 篇文章\n",
      "已處理 110000 篇文章\n",
      "已處理 120000 篇文章\n",
      "已處理 130000 篇文章\n",
      "已處理 140000 篇文章\n",
      "已處理 150000 篇文章\n",
      "已處理 160000 篇文章\n",
      "已處理 170000 篇文章\n",
      "已處理 180000 篇文章\n",
      "已處理 190000 篇文章\n",
      "已處理 200000 篇文章\n",
      "已處理 210000 篇文章\n",
      "已處理 220000 篇文章\n",
      "已處理 230000 篇文章\n",
      "已處理 240000 篇文章\n",
      "已處理 250000 篇文章\n",
      "已處理 260000 篇文章\n",
      "已處理 270000 篇文章\n",
      "已處理 280000 篇文章\n",
      "已處理 290000 篇文章\n",
      "已處理 300000 篇文章\n",
      "已處理 310000 篇文章\n",
      "已處理 320000 篇文章\n",
      "已處理 330000 篇文章\n",
      "已處理 340000 篇文章\n",
      "已處理 350000 篇文章\n",
      "已處理 360000 篇文章\n",
      "已處理 370000 篇文章\n",
      "已處理 380000 篇文章\n",
      "已處理 390000 篇文章\n",
      "已處理 400000 篇文章\n",
      "已處理 410000 篇文章\n",
      "已處理 420000 篇文章\n",
      "已處理 430000 篇文章\n",
      "已處理 440000 篇文章\n",
      "已處理 450000 篇文章\n",
      "已處理 460000 篇文章\n",
      "已處理 470000 篇文章\n",
      "已處理 480000 篇文章\n",
      "已處理 490000 篇文章\n",
      "已處理 500000 篇文章\n",
      "已處理 510000 篇文章\n",
      "已處理 520000 篇文章\n",
      "已處理 530000 篇文章\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-19 16:53:41,575 : INFO : finished iterating over Wikipedia corpus of 531556 documents with 218845379 positions (total 3163614 articles, 234043939 positions before pruning articles shorter than 50 words)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wiki_corpus = WikiCorpus(wiki_path, lemmatize=False, dictionary={''}, tokenizer_func=jieba_token)\n",
    "texts_num = 0\n",
    "\n",
    "print('start!')\n",
    "with open(output_path,'w',encoding='utf-8') as fout:\n",
    "    for text in wiki_corpus.get_texts():\n",
    "        fout.write(' '.join(text) + '\\n')\n",
    "\n",
    "        if texts_num % 10000 == 0:\n",
    "            print(\"已處理 {} 篇文章\".format(texts_num))\n",
    "        \n",
    "        texts_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set()\n",
    "with open('stop_words.txt') as fin:\n",
    "    for line in fin.readlines():\n",
    "        stopwords.add(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_output_path = 'filter_wiki.zh.txt'\n",
    "\n",
    "# with open(output_path, 'r') as fin:\n",
    "#     for line in fin.readlines():\n",
    "#         tokens = line.strip().split()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
